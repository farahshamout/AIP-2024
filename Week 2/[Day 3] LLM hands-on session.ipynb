{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uyVcC8hf9LaFRGryp4VdaeQWmSarM8Bz","timestamp":1719388349846}],"authorship_tag":"ABX9TyNtgR0jLT95+JqJi3Jt5ViP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LLMs Hands-on session\n","\n","In this session we will cover implementing a llm\n","\n"],"metadata":{"id":"EFmx7WHn36wg"}},{"cell_type":"markdown","source":["### [1] Set up your API and required keys"],"metadata":{"id":"HZLiQjVh1Kvc"}},{"cell_type":"code","source":["  api_key='sk-students-oeD5lGOU95Q5qbn1GCOBT3BlbkFJsMac71yA20yzyGjbGdcy'\n","  organizationID='org-1MmgzkePqkBwRO2aykIjqIkw'\n","  projectID='proj_oNhMie7bUL5JKObfNNr9tKNE'"],"metadata":{"id":"PAD14r-bqdM7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CM99B-oWj9Jr","collapsed":true},"outputs":[],"source":["# Install OpenAI library\n","!pip install openai"]},{"cell_type":"code","source":["# We will need these later!\n","!pip install langchain==0.0.330\n","!pip install gradio\n","!pip install transformers torch"],"metadata":{"id":"biL05qWvsXeL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from openai import OpenAI\n","\n","client = OpenAI(\n","  api_key=api_key,\n","  organization=organizationID,\n","  project=projectID,\n",")\n","\n","model = \"gpt-3.5-turbo-16k\""],"metadata":{"id":"Pac6u5F9laVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [2] Use the LLM API to perform a test with a simple prompt"],"metadata":{"id":"mjAhjQITitiZ"}},{"cell_type":"code","source":["userPrompt = \"Say this is a test\""],"metadata":{"id":"agUmgSVQjLZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["completion = client.chat.completions.create(\n","  model=model,\n","  messages=[{\"role\": \"user\", \"content\": userPrompt}],\n",")\n","\n","print(completion.choices[0].message.content)"],"metadata":{"id":"JJBaNv2boDSb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [3] multiple choice vs free text question answering\n","\n","Example of a multiple choice question:\n","\n","*   \"is the sky (a) yellow, (b)blue, or (c) green\"\n","\n","Example of a free text question\n","\n","*   \"Why is the sky blue\"\n","\n","Multiple choice answers are specific, whereas free form questions are open.\n","\n","Tasks:\n","\n","1.   come up with another exmaple of a multiple choice question and run it\n","2.   Come up with another example of a free form question and run it\n","3. Adapt your prompt to change the output of the multiple choice question. Try to output just the letter with no other text."],"metadata":{"id":"_ztYy1nrj9tQ"}},{"cell_type":"code","source":["userPrompt = \"Type your question here\""],"metadata":{"id":"Np9--0TpmYuX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["completion = client.chat.completions.create(\n","  model=model,\n","  messages=[{\"role\": \"user\", \"content\": userPrompt}],\n",")\n","\n","print(completion.choices[0].message.content)"],"metadata":{"id":"GKsvQUudmYuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [4] Zero shot, Few shot and chain of thought prompting\n","\n","https://www.promptingguide.ai/techniques/zeroshot\n","\n","https://www.promptingguide.ai/techniques/fewshot\n","\n","https://www.promptingguide.ai/techniques/cot\n","\n","Task:\n","\n","\n","1.   Using the links above, invent your own prompt examples of Zero shot, Few Shot and chain of thought prompt.\n","\n"],"metadata":{"id":"mz5rjli-mk5A"}},{"cell_type":"code","source":["# your code goes here"],"metadata":{"id":"bvHBlDfrodbP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [5] Providing context for your question (Open and closed domain question answering)\n","\n","Open domain question answering is where a LLM is allowed to use it's own training to answer a question. This means that the answer is dependent on the material that was used to train it.\n","\n","Closed domain question answering is where context is provided to the model, and the question prompt is design in such a way that the LLM is told to answer the question using only the provided context. This means that the LLM will not invent an answer based on it's own training data.\n","\n","Task:\n","\n","\n","1.   Ask a question of the LLM to answer. Try to pick a topic that the LLM may not have knowledge of (maybe a recent event) but do not provide context.\n","2. Ask the same question, but provide context.\n","3. Form a multiple choice question about the context. Does it answer it correctly?"],"metadata":{"id":"2VsfhhdonSTc"}},{"cell_type":"code","source":["# your code goes here"],"metadata":{"id":"Gbl6dF06oiKn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [6] Using the OpenAI LLM settings via the API\n","\n","The Open AI library allows for several settings to be set so that you can adapt the LLM response to the use that you need it for.\n","\n","the settings that can be changed can be found in the documentation here:\n","\n","https://platform.openai.com/docs/api-reference/chat/create\n","\n","Task:\n","\n","1. Adapt the cell below to allow the LLM to stream into this notebook.\n","2. Adapt the cell so that the output is limited to 40 tokens\n","3. Adapt the cell to change the temperature of the model to 0.9"],"metadata":{"id":"9D2YsoxipY7M"}},{"cell_type":"code","source":["stream = client.chat.completions.create(\n","    model=model,\n","    messages=[\n","      {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n","      {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n","    ],\n","    # delete this line and put the correct setting in here\n","    ,\n",")\n","for chunk in stream:\n","    if chunk.choices[0].delta.content is not None:\n","        print(chunk.choices[0].delta.content, end=\"\")"],"metadata":{"id":"vUYNefrWqySe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [6] Using frameworks and libraries to make an app\n","\n","Task:\n","\n","1. Run the cell below to make your very first chat bot!\n","2. Just like the API settings we changed before, we can change a few things about the interface below. Add a description that says \"This coding stuff isn't really that hard. I can make an app!\" [tip: search for \"description\" to find the settings in the documentation here: https://www.gradio.app/docs/gradio/chatinterface]\n","\n","Optional challenge task for coders:\n","\n","1.   Change the model to the fine-tuned model: 'ft:gpt-3.5-turbo-1106:tardis-ltd:fine-tune-for-uobd:9duA9Fke'\n","2. Within the predict function, modify it so that every output has the words \"Warning- this bot may be biased!\"\n","\n","\n"],"metadata":{"id":"VYFVXsTvre_U"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.schema import AIMessage, HumanMessage\n","import openai\n","import gradio as gr\n","\n","llm = ChatOpenAI(temperature=1.0, model=model, openai_api_key = api_key)\n","\n","def predict(message, history):\n","    history_langchain_format = []\n","    for human, ai in history:\n","        history_langchain_format.append(HumanMessage(content=human))\n","        history_langchain_format.append(AIMessage(content=ai))\n","    history_langchain_format.append(HumanMessage(content=message))\n","    gpt_response = llm(history_langchain_format)\n","    return gpt_response.content\n","\n","gr.ChatInterface(predict, title=\"My very first chat bot!!\",).launch()"],"metadata":{"id":"UZGgkLSWrzof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [7] Running a local LLM\n","\n","In this section we will download a LLM from the Hugging Face Repo using the Transformers library.\n","\n","Task:\n","\n","1.   Run the cell below\n","2.   Look at https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard or https://huggingface.co/docs/transformers/index and choose a different model to run (warning, some of them might be very large and you might run out of memory! Try to choose a small model)\n","3. Try the prompts that you tried earlier on these other models. How can you evaluate a LLMs performance?\n","4. Change and experiment with the settings that the model has access to. The documentation can be found here: https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"oXoIRzKou1K-"}},{"cell_type":"code","source":["!pip uninstall langchain -y\n","!pip install transformers torch\n","from transformers import pipeline\n","generator = pipeline('text-generation', model='gpt2')"],"metadata":{"id":"Qj0R6z0HxpfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = generator(\"Once upon a time,\",\n","                   max_length=30, num_return_sequences=1, truncation=True, pad_token_id = 50256)\n","print(\"\\n\"+ output[0][\"generated_text\"])"],"metadata":{"id":"mAxjKIhZvXuT"},"execution_count":null,"outputs":[]}]}