{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, just run the cell below to ignore unnecessary warnings for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MNIST Dataset\n",
    "\n",
    "This dataset is ubiquitous in ML and easily found in many machine learning libraries. We'll load it from `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "We'll begin with inspecting the dataset and see what it contains and what the distributions look like. It is always a good exercise to understand the data as much as possible before trying to create models to learn the mapping function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape\n",
    "\n",
    "The first thing to check in the dataset is the number of samples. Note that dataset you have just loaded is in the form of `NumPy` arrays.  \n",
    "\n",
    "**Exercise**: Using the `.shape` attribute of `NumPy` arrays to see what the data looks like, and print out the following (the first one is already implemented below):\n",
    "\n",
    "- The size of the training dataset\n",
    "- The size of the test dataset\n",
    "- The shape / form of each input sample in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Set:', x_train.shape, y_train.shape)\n",
    "print('Test Set:')\n",
    "print('Shape of input sample:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise\n",
    "\n",
    "Plot some randomly selected samples from the train and test sets to see what the dataset looks like. You can use `matplotlib` to display these images. Below is a snippet to display 25 randomly chosen samples from each set. Run it a few times to get a sense of what these images look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the training set\n",
    "\n",
    "n = 25\n",
    "\n",
    "plt.figure()\n",
    "ixs = np.random.randint(x_train.shape[0], size=n)\n",
    "for i, ix in enumerate(ixs):\n",
    "    ax = plt.subplot(int(math.sqrt(n)), int(math.sqrt(n)), i + 1, frameon=False)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(x_train[ix], cmap='gray')\n",
    "    ax.title.set_text(str(y_train[ix]))\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the test set\n",
    "\n",
    "n = 25\n",
    "\n",
    "plt.figure()\n",
    "ixs = np.random.randint(x_test.shape[0], size=n)\n",
    "for i, ix in enumerate(ixs):\n",
    "    ax = plt.subplot(int(math.sqrt(n)), int(math.sqrt(n)), i + 1, frameon=False)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(x_test[ix], cmap='gray')\n",
    "    ax.title.set_text(str(y_test[ix]))\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "\n",
    "In addition to actually visualising the data, it is useful to see some statistics on the data. In this dataset, it is useful to see how examples from each _category_ (from each digit) are distributed using a histogram. \n",
    "\n",
    "**Exercise**: Below is a snippet of code to plot a histogram (using `matplotlib`) accross the 10 _categories_ for the _training set_. Do the same for the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set distribution\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram: Training Set')\n",
    "plt.hist(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How do the two histograms compare to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task \n",
    "\n",
    "Now that we're familiar with the dataset, we can turn our attention to the task at hand. As you may have guessed, your job is to create a model that can predict the digit corresponding to every input image. \n",
    "\n",
    "We'll train several models for this task starting from simple baselines to more advanced ones that are more suitable for this task. This is an example of Supervised Learning as we have a labelled training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is this a classification or regression task? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Logistic Regression\n",
    "\n",
    "Logistic Regression is one of the simplest models one can use for a classification task. Here, we'll use a multiple classes version of logistic regression (as opposed to the standard binary classification). \n",
    "\n",
    "For this exercise, we'll use `scikit-learn` which is a good starting point for baseline models these days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset\n",
    "\n",
    "It's good to load the dataset again, just to ensure that there aren't modifications from the previous exercises that got carried over. \n",
    "\n",
    "**Exercise**: Load the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data\n",
    "\n",
    "It is often necessary to process your data before passing it to your model. This may involve normalisation, adjusting the shape, etc. \n",
    "\n",
    "In this case, we need to flatten the images. As we already saw, every sample in `x_train` and `x_test` is shaped `28x28`. However, logistic regression needs 1d array for every sample instead of a 2d array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_flat.shape)\n",
    "print(x_test_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Model\n",
    "\n",
    "**Exercise**: Use `scikit-learn`'s `LogisticRegression` to create the model and store it in `logistic_regression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "**Exercise**: Use the `fit(x, y)` function from `logisitic_regression` to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "We will use the following analysis for each model. We have provided most of the code for LR. You should understand the code and be able to reuse it for the NNs as the analysis would be the same.\n",
    "\n",
    "**Exercise**: Calculate the performance of `logistic_regression` using the `score(x, y)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a score of ~92%. An accuracy of over 92\\% is not bad at all for such a simple model and this shows how good simple models can be sometimes very performant. However, one can do significantly better with the MNIST dataset using MLPs and CNNs. \n",
    "\n",
    "Let us first see examples of where the model fails and where it succeeds. This will better highlight the _need_ for more complexity and smarter designs. \n",
    "\n",
    "**Exercise**: Use the `predict(x)` function from `logistic_regression` to get the predictions from the model for the test set and store it as `predictions`.\n",
    "\n",
    "**Exercise**: Use the `predict_proba(x)` function from `logistic_regression` to get the predicted probabilities for the test set and store it as `predicted_probs`. These will be useful for analysing failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below extracts out the positions of examples that produce different output as compared with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = np.argwhere(predictions != y_test).squeeze()\n",
    "successes = np.argwhere(predictions == y_test).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Failure Cases: {len(failures)}/{len(predictions)}')\n",
    "print(f'Successes: {len(successes)}/{len(predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing the failures\n",
    "\n",
    "By displaying examples from the test set where the model fails, we're in a much better position to understand the shortcomings and address them. Below we choose 25 random samples from the list of the failures. We can also see the probabilities predicted for a sample (randomly chosen). Run both the cells below a few times to get a sense of where and how the model fails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Failures')\n",
    "\n",
    "n = 25\n",
    "ixs = np.random.choice(failures, size=n)\n",
    "for i, ix in enumerate(ixs):\n",
    "    ax = plt.subplot(int(math.sqrt(n)), int(math.sqrt(n)), i + 1, frameon=False)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(x_test[ix], cmap='gray')\n",
    "    ax.title.set_text(f'{y_test[ix]} vs {predictions[ix]}')\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.random.choice(failures)\n",
    "\n",
    "# Show the image\n",
    "plt.figure()\n",
    "plt.title(str(y_test[ix]))\n",
    "plt.imshow(x_test[ix], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Plot the corresponding output probabilities\n",
    "plt.figure()\n",
    "plt.plot(predicted_probs[ix], 'o')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Probability')\n",
    "_ = plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Do you think we can train a better model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Multi Layer Perceptron (MLP)\n",
    "\n",
    "We will now try some neural network based models. We'll use `keras`, a simple and useful deep learning library, for creating both the MLPs and the CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data\n",
    "\n",
    "Just like in LR, MLPs need the input flattened. \n",
    "\n",
    "**Exercise**: Flatten the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Model\n",
    "\n",
    "`keras` makes it extremely easy the compose neural network layers and make architectures as you desire very easily. Below is a simple architecture already implemented with a fully connected layer of size 1024, the relu activation function, and a dropout layer to help the training. \n",
    "\n",
    "**Exercise**: Train the model on the simple architecture and perform analysis similar to what we did for LR. \n",
    "\n",
    "**Exercise**: Repeat the process for the following architecture: \n",
    "- Layer 1: Fully connected layer of size 1024 with relu activation.\n",
    "- Layer 2: Dropout of 0.25.\n",
    "- Layer 3: Fully connected layer of size 512 with relu activation. Note that unlike in the first layer, you no longer need to specify the `input_dim` as `keras` will be able to infer it from the previous layer. This is true for all subsequent layers (if any) as well. \n",
    "- Layer 4: Dropout of 0.25.\n",
    "\n",
    "**Exercise**: Try using a different activation function (eg. `'tanh'`, `'sigmoid'`).\n",
    "\n",
    "**Exercise**: Create your own architecture and repeat the same process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name='MLP')\n",
    "\n",
    "# Layer 1\n",
    "model.add(Dense(1024, input_dim=784, activation='relu'))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to compile the model. This means assembling all the layers together, telling the model which loss function to use, which optimizer to use, and which metric to use for the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference in the number of parameters in these neural networks compared with the number in our logistic regression baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "Fit the model. Remember to convert the labels into a one hot encoded format (scikit-learn did this internally for us, but we'll have to be more specific here)! You can use `keras.utils.to_categorical` for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train for 12 epochs with a batch size of 128. Feel free to experiment with more epochs or different batch sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_flat, to_categorical(y_train), epochs=12, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "**Exercise**: Perform a similar analysis as in the logistic regression by:\n",
    "- Checking the score on the test set\n",
    "- Visualising failure cases\n",
    "- Observing their output probability distributions\n",
    "\n",
    "Note: Getting the predictions out of `keras` is slightly different as compared to `scikit-learn`. Use `model.predict` to get the `predicted_probs` and use `np.argmax(predicted_probs, axis=1)` to get the actual predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Convolutional Neural Network (CNN)\n",
    "\n",
    "We still have another model up our sleeves that is in general much more suitable for training models built for images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.backend import image_data_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a bit of a keras quirk. It is fine to skip over this for now as long as you execute this cell before moving on. \n",
    "\n",
    "Since images are generally in RGB format, one will normally have 3 \"channels\" in the image. Although, our input is only in greyscale, we still have to shape the input such that there is additional dimension for channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape the input\n",
    "\n",
    "if image_data_format() == 'channels_first':\n",
    "    x_train_conv = np.expand_dims(x_train, axis=1)\n",
    "    x_test_conv = np.expand_dims(x_test, axis=1)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    x_train_conv = np.expand_dims(x_train, axis=3)\n",
    "    x_test_conv = np.expand_dims(x_test, axis=3)\n",
    "    input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Model\n",
    "\n",
    "The model we will train here will have the following hidden layers:\n",
    "- Layer 1: 2D Convolution with a 3x3 kernel, relu activation and 32 filters.\n",
    "- Layer 2: 2D Convolution with a 3x3 kernel, relu activation and 64 filters.\n",
    "- Layer 3: 2D Max Pooling with a pool size of 2x2 kernal.\n",
    "- Layer 4: Dropout of 0.25.\n",
    "- Layer 5: Flatten. \n",
    "- Layer 6: Fully Connected Layer with 128 neurons. \n",
    "- Layer 7: Dropout of 0.5. \n",
    "\n",
    "**Exercise**: Fill in the missing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name='CNN')\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "\n",
    "# Layer 2\n",
    "\n",
    "\n",
    "# Layer 3\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "\n",
    "\n",
    "# Layer 5\n",
    "model.add(Flatten())\n",
    "\n",
    "# Layer 6\n",
    "\n",
    "\n",
    "# Layer 7\n",
    "\n",
    "\n",
    "# Output Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adadelta(learning_rate=1, rho=0.95),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how despite having many more layers, the total number of parameters is less than the model in the MLP. Moreover, observe that most of the parameters are in the `Dense` layers rather than the `convolutional`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "**Exercise**: Fit the CNN to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "**Exercise**: Perform a similar analysis as in the logistic regression by:\n",
    "- Checking the score on the test set\n",
    "- Visualising failure cases\n",
    "- Observing their output probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
