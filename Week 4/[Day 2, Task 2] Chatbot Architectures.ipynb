{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/farahshamout/AIP-2024/blob/main/Week%204/%5BDay%202%2C%20Task%202%5D%20Chatbot%20Architectures.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation and Chatbot Architectures\n",
    "\n",
    "In this notebook we will work through what it takes to build a chatbot from the most basic version, all the way through to a more complicated chatbot that uses a conversational agent with tools as well as guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  #put the API key in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade gradio\n",
    "# %pip install langchain\n",
    "# %pip install langsmith\n",
    "# %pip install langchain-community\n",
    "# %pip install openai\n",
    "# %pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most basic Chatbot possible:\n",
    "\n",
    "This simply uses a user interface attached to a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import gradio as gr\n",
    "\n",
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "def predict(message, history):\n",
    "    gpt_response = llm.invoke(message)\n",
    "    return gpt_response.content\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source and clean data for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def scrape_website_text(url):\n",
    "    # Send a GET request to the Wikipedia page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all the text from paragraphs and other relevant tags\n",
    "        paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        text = ' '.join([para.get_text() for para in paragraphs])\n",
    "        \n",
    "        # Clean the text by removing special characters\n",
    "        cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "        \n",
    "        return cleaned_text.strip()\n",
    "    else:\n",
    "        return \"Failed to retrieve the page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\"https://www.cbsnews.com/news/paris-olympics-2024-200-meters-noah-lyles-kenny-bednarek-letsile-tebogo/\"\"\"  # put any website page in here\n",
    "cleaned_text = scrape_website_text(url)\n",
    "\n",
    "# you can test that this has worked using the line below (uncomment to use)\n",
    "# print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and chunk the text for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([cleaned_text])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the documents and save in a Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cell below does cost money to run, however the embeddings don't have to be done every time. Once a vector database is made, it can be saved and retrieved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "vector_database = FAISS.from_documents(texts, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test to see if we can retrieve some relevant context chunks from our document store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won gold in the men's 200m sprint at the 2024 Paris Olympics?\"\n",
    "\n",
    "print(vector_database.similarity_search(query, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include the RAG pipeline into our chatbot prototype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import gradio as gr\n",
    "\n",
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "def predict(message, history):\n",
    "    context = vector_database.similarity_search(message, 3)\n",
    "    formatted_prompt = f\"\"\"{context} \\nUse the above context to answer the follwing question: \\n{message}\"\"\"\n",
    "    gpt_response = llm.invoke(formatted_prompt)\n",
    "    return gpt_response.content\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbot with Conversational History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "import gradio as gr\n",
    "\n",
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    context = vector_database.similarity_search(message, 3)\n",
    "    formatted_prompt = f\"\"\"{context} \\nUse the above context to answer the follwing question: \\nHere is your conversation history with the user and their latest question: {history_langchain_format}\"\"\"\n",
    "    print(history_langchain_format)\n",
    "    print(context)\n",
    "    gpt_response = llm.invoke(formatted_prompt)\n",
    "    return gpt_response.content\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wikipedia\n",
    "# %pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the tool for the context retrieval function\n",
    "@tool\n",
    "def context_retreival(query: str) -> str:\n",
    "    \"\"\"This tool returns relevant context documents about the 200m sprint at the 2024 olympics in Paris. Ask this tool any question about the 2024 200m sprint finals and who won it.\"\"\"\n",
    "    \n",
    "    context = vector_database.similarity_search(query, 3)\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# defining the tool for the wiki retrieval function\n",
    "@tool\n",
    "def wiki_lookup(query: str) -> str:\n",
    "    \"\"\"This tool should be used for questions about particular people and their achievements. It returns a context document from wikipedia.\"\"\"\n",
    "    \n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    \n",
    "    context = wikipedia.run(query)\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    context_retreival,\n",
    "    wiki_lookup\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the prompt and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a user interface for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "import gradio as gr\n",
    "\n",
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "\n",
    "    gpt_response = agent_executor.invoke({\"input\": message, \"chat_history\": history_langchain_format})\n",
    "    return gpt_response['output']\n",
    "\n",
    "\n",
    "gr.ChatInterface(predict).launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
