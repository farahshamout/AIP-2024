{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation and Chatbot Architectures\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/farahshamout/AIP-2024/blob/main/Week%204/%5BDay%202%2C%20Task%202%5D%20Chatbot%20Architectures.ipynb]\n",
    "\n",
    "\n",
    "In this notebook we will work through what it takes to build a chatbot from the most basic version, all the way through to a more complicated chatbot that uses a conversational agent with tools as well as guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade gradio\n",
    "# %pip install langchain\n",
    "# %pip install langsmith\n",
    "# %pip install langchain-community\n",
    "# %pip install openai\n",
    "# %pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most basic Chatbot possible:\n",
    "\n",
    "This simply uses a user interface attached to a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  #put the API key in here\n",
    "\n",
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "def predict(message, history):\n",
    "    gpt_response = llm.invoke(message)\n",
    "    return gpt_response.content\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source and clean data for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def scrape_website_text(url):\n",
    "    # Send a GET request to the Wikipedia page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all the text from paragraphs and other relevant tags\n",
    "        paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        text = ' '.join([para.get_text() for para in paragraphs])\n",
    "        \n",
    "        # Clean the text by removing special characters\n",
    "        cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "        \n",
    "        return cleaned_text.strip()\n",
    "    else:\n",
    "        return \"Failed to retrieve the page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\"https://www.cbsnews.com/news/paris-olympics-2024-200-meters-noah-lyles-kenny-bednarek-letsile-tebogo/\"\"\"  # put any website page in here\n",
    "cleaned_text = scrape_website_text(url)\n",
    "\n",
    "# you can test that this has worked using the line below (uncomment to use)\n",
    "# print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and chunk the text for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Watch CBS News Noah Lyles of Team USA wins bronze in 200 meters after testing positive for COVID Botswanas Letsile Tebogo claims gold \n",
      "    By\n",
      "                        \n",
      "                      Allison Elyse Gualtieri\n",
      " \n",
      "Updated on  August 8 2024  819 PM EDT\n",
      "           CBS News'\n",
      "page_content='Noah Lyles sought to follow up his gold medal in the 100 meters with a matching one in the 200 meters On Thursday the favorite instead claimed the bronze in the 2024 Paris Olympics  and revealed that he had been diagnosed with COVID19 two days earlier  He finished behind Letsile Tebogo of Botswana who won gold and fellow American Kenny Bednarek who garnered the silver  Lyles a 27yearold from northern Virginia said in an interview with NBC after the race that he woke up early Tuesday feeling really horrible  I knew it was more than just being sore from the 100 he said Woke up the doctors and we tested and unfortunately it came up that I was positive for COVID  Lyles was seen almost immediately after the race asking for water and going down to one knee He sat on the side of the track for an extended period of time as medical personnel attended to him Photographs and video from after the event showed him being taken off the track in a wheelchair  He said in the interview Its'\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([cleaned_text])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the documents and save in a Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cell below does cost money to run, however the embeddings don't have to be done every time. Once a vector database is made, it can be saved and retrieved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "vector_database = FAISS.from_documents(texts, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test to see if we can retrieve some relevant context chunks from our document store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='by a mere five thousandths of a second American Fred Kerley rounded out the medals in photo finish with the top four finishers separated by just 04 seconds and the top seven just 09  a literal blink of an eye Bednarek claimed the silver in the 200 meters in Tokyo but fell behind in the 100 meters in Paris finishing seventh at the Stade de France Lyles also took home the bronze in the 200 meters in Tokyo He added the 100meter sprint for the Paris Games which paid off earlier this week The 2024 Summer Games have been good for Team USA which led the medal count in athletics  the track and field events  going into competition Friday six gold seven silver and six bronze\\xa0 2024 Summer Olympics in Paris Allison Elyse Gualtieri is a Senior News Editor for CBSNewscom working on a wide variety of subjects including crime longerform features and feelgood news She previously worked for the Washington Examiner and US News and World Report among other outlets'), Document(page_content='Noah Lyles sought to follow up his gold medal in the 100 meters with a matching one in the 200 meters On Thursday the favorite instead claimed the bronze in the\\xa02024 Paris Olympics\\xa0 and revealed that he had been diagnosed with COVID19 two days earlier\\xa0 He finished behind Letsile Tebogo of Botswana who won gold and fellow American Kenny Bednarek who garnered the silver\\xa0 Lyles a 27yearold from northern Virginia said in an interview with NBC after the race that he woke up early Tuesday feeling really horrible  I knew it was more than just being sore from the 100 he said Woke up the doctors and we tested and unfortunately it came up that I was positive for COVID\\xa0 Lyles was seen almost immediately after the race asking for water and going down to one knee He sat on the side of the track for an extended period of time as medical personnel attended to him Photographs and video from after the event showed him being taken off the track in a wheelchair\\xa0 He said in the interview Its'), Document(page_content='one more race the 4x100meters relay but he told NBC after the 200 meters he wasnt sure if he would compete in that event Im feeling more on the side of letting Team USA do their thing Theyve proven with great certainty they can handle it without me he said If thats the case coming off today Im perfectly fine saying Hey you guys do your thing You have more than enough speed to handle it and get the gold medal \\xa0 Later on Thursday Lyles appeared to say in an Instagram post that he would not compete in that event writing I believe this will be the end of my 2024 Olympics It is not the Olympic I dreamed of but it has left me with so much Joy in my heart Lyles wrote I hope everyone enjoyed the show Whether you were rooting for me or against me you have to admit you watched didnt you See you next time On Sunday Lyles narrowly edged Kishane Thompson to capture the 100meter gold besting the Jamaican by a mere five thousandths of a second American Fred Kerley rounded out the medals in photo')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Who won gold in the men's 200m sprint at the 2024 Paris Olympics?\"\n",
    "\n",
    "print(vector_database.similarity_search(query, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include the RAG pipeline into our chatbot prototype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  #put the API key in here\n",
    "\n",
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "def predict(message, history):\n",
    "    context = vector_database.similarity_search(message, 3)\n",
    "    formatted_prompt = f\"\"\"{context} \\nUse the above context to answer the follwing question: \\n{message}\"\"\"\n",
    "    gpt_response = llm.invoke(formatted_prompt)\n",
    "    return gpt_response.content\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbot with Conversational History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  #put the API key in here\n",
    "\n",
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    context = vector_database.similarity_search(message, 3)\n",
    "    formatted_prompt = f\"\"\"{context} \\nUse the above context to answer the follwing question: \\nHere is your conversation history with the user and their latest question: {history_langchain_format}\"\"\"\n",
    "    print(history_langchain_format)\n",
    "    print(context)\n",
    "    gpt_response = llm.invoke(formatted_prompt)\n",
    "    return gpt_response.content\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wikipedia\n",
    "# %pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the tool for the context retrieval function\n",
    "@tool\n",
    "def context_retreival(query: str) -> str:\n",
    "    \"\"\"This tool returns relevant context documents about the 200m sprint at the 2024 olympics in Paris. Ask this tool any question about the 2024 200m sprint finals and who won it.\"\"\"\n",
    "    \n",
    "    context = vector_database.similarity_search(query, 3)\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# defining the tool for the wiki retrieval function\n",
    "@tool\n",
    "def wiki_lookup(query: str) -> str:\n",
    "    \"\"\"This tool should be used for questions about particular people and their achievements. It returns a context document from wikipedia.\"\"\"\n",
    "    \n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    \n",
    "    context = wikipedia.run(query)\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    context_retreival,\n",
    "    wiki_lookup\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the prompt and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aymoncollins/Documents/Projects/Dubai AI /Scripts/.venv/lib/python3.10/site-packages/langchain/hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a user interface for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aymoncollins/Documents/Projects/Dubai AI /Scripts/.venv/lib/python3.10/site-packages/gradio/routes.py:1188: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "/Users/aymoncollins/Documents/Projects/Dubai AI /Scripts/.venv/lib/python3.10/site-packages/fastapi/applications.py:4495: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  return self.router.on_event(event_type)\n",
      "/Users/aymoncollins/Documents/Projects/Dubai AI /Scripts/.venv/lib/python3.10/site-packages/gradio/routes.py:1188: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "/Users/aymoncollins/Documents/Projects/Dubai AI /Scripts/.venv/lib/python3.10/site-packages/fastapi/applications.py:4495: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  return self.router.on_event(event_type)\n",
      "/Users/aymoncollins/Documents/Projects/Dubai AI /Scripts/.venv/lib/python3.10/site-packages/gradio/routes.py:1188: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "/Users/aymoncollins/Documents/Projects/Dubai AI /Scripts/.venv/lib/python3.10/site-packages/fastapi/applications.py:4495: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  return self.router.on_event(event_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aymoncollins/Documents/Projects/Dubai AI /Scripts/.venv/lib/python3.10/site-packages/starlette/templating.py:178: DeprecationWarning: The `name` is not the first parameter anymore. The first parameter should be the `Request` instance.\n",
      "Replace `TemplateResponse(name, {\"request\": request})` by `TemplateResponse(request, name)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  #put the API key in here\n",
    "\n",
    "llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-16k')\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "\n",
    "    gpt_response = agent_executor.invoke({\"input\": message, \"chat_history\": history_langchain_format})\n",
    "    return gpt_response['output']\n",
    "\n",
    "\n",
    "gr.ChatInterface(predict).launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
